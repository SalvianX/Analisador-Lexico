#Analisador lexico

import tkinter as tk
from tkinter import scrolledtext
from ply import lex

# lista das palavras-chave reservadas para interpretar as linguagens Python
reserved = {
    'if': 'IF',
    'else': 'ELSE',
    'elif': 'ELIF',
    'for': 'FOR',
    'while': 'WHILE',
    'def': 'DEF',
    'return': 'RETURN',
    'True': 'TRUE',
    'False': 'FALSE',
    'None': 'NONE',
    'and': 'AND',
    'or': 'OR',
    'not': 'NOT',
}

# lista dos  tokens c
tokens = [
    'NUMERO',
    'MAIS',
    'MENOS',
    'VEZES',
    'DIVIDIDO',
    'PARENTESE_ESQ',
    'PARENTESE_DIR',
    'IDENTIFICADOR',
    'STRING',
    'COMENTARIO',
    'MENOR',
    'MENOR_IGUAL',
    'MAIOR',
    'MAIOR_IGUAL',
    'IGUAL',
    'DIFERENTE',
    'ATRIBUICAO',
    'DOIS_PONTOS',
    'VIRGULA',
    'PONTO_E_VIRGULA',
    'COLCHETE_ESQ', 
    'COLCHETE_DIR',
    'PONTO',  
    'CHAVE_ESQ',
    'CHAVE_DIR',
    'ASPAS' ,


] + list(reserved.values())

# função dos tokens
t_MAIS = r'\+'
t_MENOS = r'-'
t_VEZES = r'\*'
t_DIVIDIDO = r'/'
t_PARENTESE_ESQ = r'\('
t_PARENTESE_DIR = r'\)'
t_MENOR = r'<'
t_MENOR_IGUAL = r'<='
t_MAIOR = r'>'
t_MAIOR_IGUAL = r'>='
t_IGUAL = r'=='
t_DIFERENTE = r'!='
t_ATRIBUICAO = r'='
t_DOIS_PONTOS = r':'
t_VIRGULA = r','
t_ASPAS = r'\''
t_PONTO_E_VIRGULA = r';'
t_COLCHETE_ESQ = r'\['  
t_COLCHETE_DIR = r'\]' 
t_PONTO = r'\.' 
t_CHAVE_ESQ = r'\{'
t_CHAVE_DIR = r'\}'

# função para os números inteiros
def t_NUMERO(t):
    r'\d+'
    t.value = int(t.value)
    return t

# função para identificadores e as palavras-chave
def t_IDENTIFICADOR(t):
    r'[a-zA-Z_][a-zA-Z0-9_]*'
    t.type = reserved.get(t.value, 'IDENTIFICADOR')
    return t

# função para as strings
def t_STRING(t):
    r'\"([^\\\n]|(\\.))*?\"'
    t.value = t.value[1:-1]  # Remover aspas
    return t

# função pros  comentários
def t_COMENTARIO(t):
    r'\#.*'
    pass

# Ignorar os espaço em branco e a quebra de linahs
t_ignore = ' \t\n'

# Regra para tratamento de erros
def t_error(t):
    global saida_erros
    erro = f"Erro: Caractere ilegal '{t.value[0]}'\n"
    saida_erros.insert(tk.END, erro)
    t.lexer.skip(1)
    return t

# construir o lexer
lexer = lex.lex()

# função para analisar o léxico
def analisar():
    global saida_tokens
    global saida_erros
    codigo = entrada_codigo.get("1.0", tk.END)
    lexer.input(codigo)
    saida_tokens.delete("1.0", tk.END)
    saida_erros.delete("1.0", tk.END)  # limpar mensagens de erro
    while True:
        tok = lexer.token()
        if not tok:
            break
        if tok.type != 'COMENTARIO':  # ignorar os comentários dos programas utilizados para teste
            if tok.type == 'IDENTIFICADOR':
                saida_tokens.insert(tk.END, f"{tok.type}({tok.value})\n")
            elif tok.type == 'STRING':
                saida_tokens.insert(tk.END, f"{tok.type}('{tok.value}')\n")
            else:
                saida_tokens.insert(tk.END, f"{tok.type}\n")

# criar a janela
janela = tk.Tk()
janela.title("Analisador Léxico Python")

# criar área de entrada de código
entrada_codigo = scrolledtext.ScrolledText(janela, width=40, height=10)
entrada_codigo.pack(padx=10, pady=10)

# botão análisar o código
botao_analisar = tk.Button(janela, text="Analisar", command=analisar)
botao_analisar.pack(pady=5)

# janela de exibição dos tokens
saida_tokens = scrolledtext.ScrolledText(janela, width=40, height=10)
saida_tokens.pack(padx=10, pady=10)

# janela de exibição dos erros
saida_erros = scrolledtext.ScrolledText(janela, width=40, height=5, bg="pink")
saida_erros.pack(padx=10, pady=10)

# rodar o gerador de analise lexica
janela.mainloop()
